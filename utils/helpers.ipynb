{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e2db3a-0637-4aa1-9f88-3dcba1a0cad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33f3c95-f270-428a-b2b9-e4596ddff4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_volume_files(volume_path: str, column_mapping_dict: dict = {}, **options) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads JSON, CSV, XLSX as Spark DataFrame from Volumes,\n",
    "    cast column types using schema and mapping dictionary to convert all column names to snake case.\n",
    "\n",
    "    Args:\n",
    "        volume_path (str): Path to the volume.\n",
    "        schema (StructType): Schema for the DataFrame.\n",
    "        column_mapping_dict (dict): Dictionary for column name mapping.\n",
    "        **options: Keyword arguments for Spark DataFrame options.\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    default_options = {\"header\": \"true\",\n",
    "                       \"inferSchema\": \"true\"\n",
    "                    }\n",
    "    default_options.update(options)\n",
    "\n",
    "    if volume_path.endswith(\".csv\"):\n",
    "        df = spark.read.format(\"csv\").options(**default_options).load(volume_path)\n",
    "    elif volume_path.endswith(\".json\"):\n",
    "        df = spark.read.format(\"json\").options(**default_options).load(volume_path)\n",
    "    elif volume_path.endswith(\".xlsx\"):\n",
    "        df = spark.read.options(**default_options).excel(volume_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {volume_path}\")\n",
    "\n",
    "    df = df.withColumnsRenamed(column_mapping_dict)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656bc4f6-719f-469a-880d-5c4b230857b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_columns(df: DataFrame, schema: None):\n",
    "    return df.select(*[col(f.name).cast(f.dataType).alias(f.name) for f in schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ebe691-5635-4a1b-9871-9a13539816c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def raw_date_write(df, full_table_name):\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helpers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
