{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875a8eed-6faa-4faa-a94b-447965d893bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def validate_columns(df: DataFrame, columns_expected_datatype: dict):\n",
    "    \"\"\"\n",
    "    Validates the type of a column in a DataFrame.\n",
    "    data type examples:\n",
    "        - \"string\"\n",
    "        - \"int\"\n",
    "        - \"double\"\n",
    "        - \"date\"\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        columns_expected_datatype: dictionary of column names and expected data types\n",
    "\n",
    "    Returns:\n",
    "        valid_df: rows where all columns match expected data type\n",
    "        invalid_df: rows where any column does not match expected data type\n",
    "    \"\"\"\n",
    "\n",
    "    conditions = []\n",
    "\n",
    "    for column, expected_type in columns_expected_datatype.items():\n",
    "        # Flag column\n",
    "        if expected_type == \"date\":\n",
    "            # df = df.withColumn(column, expr(f\"to_date({column}, 'd/M/yyyy')\"))\n",
    "            df = df.withColumn(column, expr(f\"coalesce(to_date({column}, 'd/M/yyyy'), to_date({column}, 'yyyy-MM-dd'))\"))\n",
    "        else:\n",
    "            df = df.withColumn(column, expr(f\"try_cast({column} as {expected_type})\"))\n",
    "\n",
    "        conditions.append(col(column).isNotNull())\n",
    "\n",
    "    combined_conditions = conditions[0]\n",
    "    for condition in conditions[1:]:\n",
    "        combined_conditions = combined_conditions & condition\n",
    "        \n",
    "    valid_df = df.filter(combined_conditions)\n",
    "    invalid_df = df.filter(~combined_conditions)\n",
    "\n",
    "    return valid_df, invalid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7133cd4d-0d2b-4b39-8f56-bac4136e5ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_primary_key_unique(df: DataFrame, primary_key: list):\n",
    "    \"\"\"\n",
    "    Validate primary key uniqueness and returns duplicate rows\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        primary_key: single or list of columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing duplicate rows\n",
    "        Or message - No duplicates found\n",
    "    \"\"\"\n",
    "    total_count = df.count()\n",
    "    unique_count = df.select(primary_key).dropDuplicates().count()\n",
    "\n",
    "    if total_count != unique_count:\n",
    "        # Get duplicate keys\n",
    "        df_dupliicates = df.groupBy(primary_key).count().filter(\"count > 1\").drop(\"count\")\n",
    "        # Join to get duplicate rows\n",
    "        dup_rows = df.join(df_dupliicates, primary_key)\n",
    "        return dup_rows\n",
    "    else:\n",
    "        return \"No duplicates found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e120de6-baf5-4880-a1e5-f7cf2c4fe300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def null_check(df):\n",
    "    \"\"\"\n",
    "    Checks for null values in a DataFrame\n",
    "    and returns a DataFrame with the count of null values\n",
    "    for columns with atleast one null value.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with count of null values for columns with atleast one null value\n",
    "    \"\"\"\n",
    "\n",
    "    # Get count of null values in each column\n",
    "    df_null_count = df.select([\n",
    "                            count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "                        ])\n",
    "    \n",
    "    # Convert to dict and get columns with atleast one null value\n",
    "    null_count_dict = df_null_count.first().asDict()\n",
    "    keep_cols = [k for k, v in null_count_dict.items() if v != 0]\n",
    "\n",
    "    return df_null_count.select(*keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998630af-3aae-4fd5-a369-184f4d762a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dq_compare(column_name: str, operator: str, value: str):\n",
    "    \"\"\"\n",
    "    Checks if a column in a DataFrame satisfies a comparison condition.\n",
    "    \n",
    "    Args:\n",
    "    df: Spark DataFrame\n",
    "    column_name: column name to compare\n",
    "    operator: operator to compare with\n",
    "\n",
    "    Returns:\n",
    "        Boolean\n",
    "    \"\"\"\n",
    "\n",
    "    c = col(column_name)\n",
    "    if operator == \">\":\n",
    "        return c > value\n",
    "    elif operator == \"<\":\n",
    "        return c < value\n",
    "    elif operator == \">=\":\n",
    "        return c >= value\n",
    "    elif operator == \"<=\":\n",
    "        return c <= value\n",
    "    elif operator == \"==\":\n",
    "        return c == value\n",
    "    elif operator == \"!=\":\n",
    "        return c != value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator. Use one of: >, <, >=, <=, ==, !=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617ec395-0574-4169-b1be-153eb29edd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dq_compare_columns(df: DataFrame, column_name1: str, column_name2: str, operator: str, flag_column_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Checks if a column in a DataFrame satisfies a comparison condition.\n",
    "\n",
    "    Args:\n",
    "    df: Spark DataFrame\n",
    "    column_name: column name to compare\n",
    "    operator: operator to compare with\n",
    "\n",
    "    Returns:\n",
    "        Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    col1 = col(column_name1)\n",
    "    col2 = col(column_name2)\n",
    "\n",
    "    if operator == \">\":\n",
    "        expr = col1 > col2\n",
    "    elif operator == \"<\":\n",
    "        expr = col1 < col2\n",
    "    elif operator == \">=\":\n",
    "        expr = col1 >= col2\n",
    "    elif operator == \"<=\":\n",
    "        expr = col1 <= col2\n",
    "    elif operator == \"==\":\n",
    "        expr = col1 == col2\n",
    "    elif operator == \"!=\":\n",
    "        expr = col1 != col2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator. Use one of: >, <, >=, <=, ==, !=\")\n",
    "\n",
    "    return df.withColumn(flag_column_name, expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24766b08-808a-44eb-8911-b44aeeefc45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dq_email_check(df: DataFrame, email_col: str, flag_column_name: str):\n",
    "    \"\"\"\n",
    "    Checks if a column in a DataFrame contains valid email address\n",
    "\n",
    "    Args:\n",
    "    df: Spark DataFrame\n",
    "    email_col: column name to check\n",
    "    flag_column_name: column name to flag\n",
    "\n",
    "    Returns:\n",
    "        Dataframe\n",
    "    \"\"\"\n",
    "    email_check = r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\"\n",
    "    return df.withColumn(flag_column_name, col(email_col).rlike(email_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b483ffb8-f0fc-47bc-9ac2-7c5439df3708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def missing_reference_check(df, df_ref, join_col_name):\n",
    "    \"\"\"\n",
    "    Check if reference data is not in table yet, but it exists in transactional data.\n",
    "\n",
    "    Args:\n",
    "        df: Transactional/Fact DataFrame\n",
    "        df_ref: Reference/Dimension DataFrame\n",
    "        join_col_name: column name to join on\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"\n",
    "    return df.join(df_ref, join_col_name, \"left\").drop(df_ref[join_col_name]).filter(col(join_col_name).isNull())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rules.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
